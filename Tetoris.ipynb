{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9110d7-f6ec-43cd-8c54-c30d3d324497",
   "metadata": {},
   "source": [
    "# Teto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73bb67-0bd5-4d8b-aeef-209672056629",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bf7e8-082d-48d4-9fce-e8800e2fb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "teto = \"\"\"                                          =**++**+****=+==.                                         \n",
    "                                    .***************#**++++++==-                                    \n",
    "                                  **#***##*#*++*****+**++++==+======.                               \n",
    "                               :*#*#*##*****#*#******+++++++++++==+====:                            \n",
    "                             .*####******************++++*+=+==+=+=======-                          \n",
    "                            #%%#*****+***#**********+*++++==+=+===+========:                        \n",
    "                          #%%*#*#*##***********=++**+**+*+++=+++==+====---===                       \n",
    "                        :#**###***********+*++*=**+*++++***=++====+==========-                      \n",
    "                       +********+***+***********+****+++++==+++==============-:                     \n",
    "                .==**#++*#*********************+**+*+++*++++++++===========-=---                    \n",
    "         .=++*=++***#%+#********+*+***+********+****====+==================-==---==---:.            \n",
    "    +*++++*++**++**%%****+********+***+****+*++*****++*++=================----==--*+*+=====-=---:   \n",
    "    ****+==+**++**%%%%************+***++**++==*+**+*++*++==++====+========--==----********+==-==-:: \n",
    "    **++=+=++==***%%*********+++++++*++=++=+++++++*+**++===================-=----:+***+===++===--:  \n",
    "     #*++#**=*+=+*#%*#*****+***+**=**+=++++=+++=+=*+==++=+================-==-----*#*++++=+===+=-.  \n",
    "   =+*+=++**#%**++#**#******+*+=***+++=+*+=++++++***++++++#================-----=-:*=+====--===-==  \n",
    "  *+*+++++=**##%@@%******+*****+***+**+++++==#=+********+=-*=+============-----:--:-*+*++===+===--  \n",
    " .***+=======**%%@@.******************+**+++=****+*++**+++.:*=============----------==***#=====+=:  \n",
    " .****+===-+**#%%@@@+**********+*****+****+:=+*++++*=*+**:..:+*=============------:@%%##*===:==--=  \n",
    "  +**===+=++***%%@@%*****+***************+::=+*++*+++==++....:=#=======-===-=--=--*@%#*+=+===::==-  \n",
    "   +++=====+++#@@@@*+******************+=:::=+*=++++====-.. ...-*-======----=--=--:%#**=========--  \n",
    "      %%%%%@#*#%%%#*#****************++::%#-=+*+++++==+=... .:*+:*====---=--====--::#%*==-=====--   \n",
    "     +***+++***#%%*********#*********=:::::-=%*=++++==+.....=.....-=-==--=-=====---#%%#*==*+*==:    \n",
    "     ***==+=****#%%%@*********#*****+:::::-=+*+=++++==.............:*--======---%%#%#*+=++*====     \n",
    "      **+===****#%@@@**************%%##%%%++%*++**++=.......*.+**#***#%---==---:%%###*+**+*=+=-.    \n",
    "        *++===*+*%%%@*******+****%%%%-::=%@@%+=+++==........%#*%.  #*%*=---=----**%%##***+=====     \n",
    "        =++*****#%%%%*******+++*@#%@@.::=@%%*=#++=......... .*#=..:%%-#@..%=#==****+++*#***+==.     \n",
    "         +====++*##%#%#++***=*=-@-@@@@::@@%%==+=............***%..#%%:*..*-:%##*#*#****+====:       \n",
    "                      *=:%%%=%=-=-%@@@@%%@%%*:..............*#%*#%%%%...:*-    .:**##***==:         \n",
    "                          %%**+-:::%%%%%%%#%+::.............*=+*****....%-                          \n",
    "                          %%#+#--:::%%%%#***:..:.............*==+*#....%=                           \n",
    "                          .###=*=-::::::=::::.........................%-                            \n",
    "                            -##=%*=::::::::.::......................:*=.                            \n",
    "                              :+*##*:-:::::::::..@.:..=#:.........=====--.                          \n",
    "                           .===----:#+==--::::::::::::::::.::::++==*%-:::+*#%*                      \n",
    "                        .%%#*::::.:::#%%%%%%##*=-----:=***=*****+:::-:..=**%%%%#@#                  \n",
    "                     @%%@%##*+-....::*%%%%%##**###******++******+.:::::=**##%**@#@#*@#-             \n",
    "                 +@%%@%@@#***+=:....:%%%%%%%%##**##%**==*****%%*#=*=-:+**%%%#%@#@**%*%#+*           \n",
    "             ***#%#%@#********=+...:#@%%%%@@%#%***%*%*#*#%****%##%%@%#%#%%%%%%###@%**%*:.           \n",
    "            :*=+**************#*:%@@@@@@@%%%#@#**++==+**%**%**@##%%@@%%@%%%%%%%%%#*****.            \n",
    "             ---=+=+*+******%%%@@@@@@@@%%%%%%%##*+%*%****#****@##@@%%%%%   %%%%%%%##+* .            \n",
    "             =:::===+***#%:  %@@@%@@@@@%@%%%@###*++++****%%%****@%#%%%%%      .%@%#**..             \n",
    "              =:::===##-     %@@@%%%@@@@%%%%%%%##*@@+*****#%%%%%@%@@%###=         **.               \n",
    "                ::=*+        %%@@@@@%%#%%@%%%%%%%*%%#%%@@@%%%*#*****%***#                           \n",
    "                              @@@%%@@*%%@%@#%%%%%%%@@@@%##*#%%%@%%%%*%%#**+                         \n",
    "                              %%@@@%%%@@#%%%%@%@@@@%%%%@@@@@@@%%#****#+*%#+*                        \n",
    "                              +%%@*##%%%@@%@@@@@%%%@@@@@@@@@%%%@@@*@-                               \n",
    "                              =@@%@@@###%%%%@@@%@%@@@@@@@@@%@@@@@%%@@%                              \n",
    "                            .#****=++*@%*%%%%@%@@@@@@%@@%%@##***+==+#@                              \n",
    "                           -#***++====++=:*##@@%%@@@@@@%: %##*++=====+:                             \n",
    "                           %##**++=====++   %@%%%%@%%%%  #%#**++=====+%                             \n",
    "                           %%#***+++====+.    +%%%%%-    %%%#**++===++*                             \n",
    "                           #%##***++++++*                 %%%#**++++++*                             \n",
    "                            %%##****++*+                  =%%##****+++                              \n",
    "                             .%%#******                     -%%#*****                               \n",
    "                                :%%%=                                                               \"\"\"\n",
    "print(teto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7f276-0684-47c6-9086-f84631e789c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class TetrisEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TetrisEnv, self).__init__()\n",
    "        #self.action_space = spaces.Discrete(7)\n",
    "        self.action_space = spaces.Discrete(4)   # Simplified Controls\n",
    "\n",
    "        # Observation space:\n",
    "        # Grid:               10 column x 20 row bits, one per cell\n",
    "        # Placing Piece Grid: identical to Grid above\n",
    "        # Piece Type:         7 bits, 1-hot\n",
    "        # Rotation:           4 bits, 1-hot\n",
    "        # Position:           x and y position of\n",
    "        # Held Piece:         8 bits, 1-hot\n",
    "        # Held this turn:     1 bit\n",
    "        # Next Pieces:        3 x 7 bits, 1-hot each\n",
    "        # Incoming:           # of rows\n",
    "        \n",
    "        #self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(477,), dtype=np.float32)\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(400,), dtype=np.float32) # Minimal observation space\n",
    "        \n",
    "        self.reward = 0.0\n",
    "        self.firstGame = True\n",
    "        self.terminated = False\n",
    "        self.truncated = False\n",
    "\n",
    "        self.board_arr = None\n",
    "        self.placing_board_arr = None\n",
    "        self.placing_onehot = None\n",
    "        self.rotation_onehot = None\n",
    "        self.pos = None\n",
    "        self.held_onehot = None\n",
    "        self.hasheld_bit = None\n",
    "        self.next_pieces_threehot = None\n",
    "        self.incoming_severity = None\n",
    "\n",
    "        self.prev_board_arr = None\n",
    "        self.prev_placing_board_arr = None\n",
    "        self.prev_placing_onehot = None\n",
    "        self.prev_rotation_onehot = None\n",
    "        self.prev_pos = None\n",
    "        self.prev_held_onehot = None\n",
    "        self.prev_hasheld_bit = None\n",
    "        self.prev_next_pieces_threehot = None\n",
    "        self.prev_incoming_severity = None\n",
    "\n",
    "        self.stepend_timestamp = None\n",
    "        self.stepbegin_timestamp = None\n",
    "        \n",
    "        self.stdioend_timestmap = None\n",
    "        self.stdiobegin_timestamp = None\n",
    "\n",
    "        self.renderend_timestamp = None\n",
    "        self.renderbegin_timestamp = None\n",
    "\n",
    "\n",
    "    def save_previous_observation_components(self):\n",
    "        self.prev_board_arr = self.board_arr\n",
    "        self.prev_placing_board_arr = self.placing_board_arr\n",
    "        self.prev_placing_onehot = self.placing_onehot\n",
    "        self.prev_rotation_onehot = self.rotation_onehot\n",
    "        self.prev_pos = self.pos\n",
    "        self.prev_held_onehot = self.held_onehot\n",
    "        self.prev_hasheld_bit = self.hasheld_bit\n",
    "        self.prev_next_pieces_threehot = self.next_pieces_threehot\n",
    "        self.prev_incoming_severity = self.incoming_severity\n",
    "    \n",
    "\n",
    "\n",
    "    # By ChatGPT\n",
    "    def count_enclosed_regions(self, board):\n",
    "        \"\"\"\n",
    "        Counts the number of empty regions fully enclosed (orthogonally) by 1s \n",
    "        and not touching the top row.\n",
    "        board: 2D numpy array of shape (20, 10), 0 = empty, 1 = filled\n",
    "        \"\"\"\n",
    "        rows, cols = board.shape\n",
    "        visited = np.zeros_like(board, dtype=bool)\n",
    "        enclosed_count = 0\n",
    "    \n",
    "        # Directions for orthogonal neighbors: up, down, left, right\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if board[r, c] == 0 and not visited[r, c]:\n",
    "                    # Start BFS for this empty region\n",
    "                    queue = deque()\n",
    "                    queue.append((r, c))\n",
    "                    visited[r, c] = True\n",
    "                    touches_top = (r == 0)\n",
    "                    enclosed = True\n",
    "    \n",
    "                    while queue:\n",
    "                        cr, cc = queue.popleft()\n",
    "                        for dr, dc in directions:\n",
    "                            nr, nc = cr + dr, cc + dc\n",
    "                            if 0 <= nr < rows and 0 <= nc < cols:\n",
    "                                if board[nr, nc] == 0 and not visited[nr, nc]:\n",
    "                                    visited[nr, nc] = True\n",
    "                                    queue.append((nr, nc))\n",
    "                            else:\n",
    "                                # Reaching outside the board (shouldn't happen in Tetris), still enclosed\n",
    "                                continue\n",
    "                        if cr == 0:\n",
    "                            touches_top = True\n",
    "    \n",
    "                    # Only count if region doesn't touch top\n",
    "                    if not touches_top:\n",
    "                        enclosed_count += 1\n",
    "    \n",
    "        return enclosed_count\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def parse_observations(self):\n",
    "\n",
    "        # Board Input\n",
    "        board = input().split()\n",
    "        self.renderend_timestamp = time.perf_counter()\n",
    "        render_time = self.renderend_timestamp - self.renderbegin_timestamp\n",
    "        print(f\"Render took about: {(render_time*1000):.4f} ms\")\n",
    "        \n",
    "        board_ints = [int(board[i+1]) for i in range(20)]\n",
    "        #print(board_ints, flush=True)\n",
    "        arr = np.zeros((20, 10), dtype=np.float32)\n",
    "        for y, row_val in enumerate(board_ints):\n",
    "            bits = np.binary_repr(row_val, width=10)\n",
    "            arr[y] = np.array(list(bits), dtype=np.float32)\n",
    "        self.board_arr = arr.flatten()\n",
    "        #print(self.board_arr, flush=True)\n",
    "\n",
    "        # placing_Board Input\n",
    "        placing_board = input().split()\n",
    "        placing_board_ints = [int(placing_board[i+1]) for i in range(20)]\n",
    "        #print(placing_board_ints, flush=True)\n",
    "        placing_arr = np.zeros((20, 10), dtype=np.float32)\n",
    "        for y, row_val in enumerate(placing_board_ints):\n",
    "            placing_bits = np.binary_repr(row_val, width=10)\n",
    "            placing_arr[y] = np.array(list(placing_bits), dtype=np.float32)\n",
    "        self.placing_board_arr = placing_arr.flatten()\n",
    "        #print(self.board_arr, flush=True)\n",
    "\n",
    "        # Placing Input\n",
    "        placing = input().split()\n",
    "        self.placing_onehot = np.zeros(7, dtype=np.float32)\n",
    "        match placing[1]:\n",
    "            case \"I\": self.placing_onehot[0] = 1\n",
    "            case \"J\": self.placing_onehot[1] = 1\n",
    "            case \"L\": self.placing_onehot[2] = 1\n",
    "            case \"O\": self.placing_onehot[3] = 1\n",
    "            case \"S\": self.placing_onehot[4] = 1\n",
    "            case \"T\": self.placing_onehot[5] = 1\n",
    "            case \"Z\": self.placing_onehot[6] = 1\n",
    "        #print(self.placing_onehot, flush=True)\n",
    "\n",
    "        # Rotation Input\n",
    "        rotation = input().split()\n",
    "        self.rotation_onehot = np.eye(4)[int(rotation[1])]\n",
    "        #print(self.rotation_onehot)\n",
    "\n",
    "        # Position Input\n",
    "        position = input().split()\n",
    "        posx = np.eye(10)[int(position[1])]\n",
    "        posy = np.eye(25)[min(24,int(position[2]))]\n",
    "        #print(posx)\n",
    "        #print(posy)\n",
    "        self.pos = np.concatenate([posx, posy])\n",
    "        #print(self.pos, flush=True)\n",
    "\n",
    "        # Held Input\n",
    "        held = input().split()\n",
    "        self.held_onehot = np.zeros(8, dtype=np.float32)\n",
    "        match held[1]:\n",
    "            case \"I\": self.held_onehot[0] = 1\n",
    "            case \"J\": self.held_onehot[1] = 1\n",
    "            case \"L\": self.held_onehot[2] = 1\n",
    "            case \"O\": self.held_onehot[3] = 1\n",
    "            case \"S\": self.held_onehot[4] = 1\n",
    "            case \"T\": self.held_onehot[5] = 1\n",
    "            case \"Z\": self.held_onehot[6] = 1\n",
    "            case \"0\": self.held_onehot[7] = 1\n",
    "        #print(self.held_onehot, flush=True)\n",
    "\n",
    "        # Has Held Input\n",
    "        hasheld = input().split()\n",
    "        self.hasheld_bit = np.array([1.0 if hasheld[1]==\"true\" else 0.0], dtype=np.float32)\n",
    "        #print(self.hasheld_bit, flush=True)\n",
    "\n",
    "        # Next Pieces Input\n",
    "        self.next_pieces_threehot = np.zeros((3,7), dtype=np.float32)\n",
    "        nextPieces = input().split()\n",
    "        for i in range(3):\n",
    "            match nextPieces[1+i]:\n",
    "                case \"I\": self.next_pieces_threehot[i,0] = 1\n",
    "                case \"J\": self.next_pieces_threehot[i,1] = 1\n",
    "                case \"L\": self.next_pieces_threehot[i,2] = 1\n",
    "                case \"O\": self.next_pieces_threehot[i,3] = 1\n",
    "                case \"S\": self.next_pieces_threehot[i,4] = 1\n",
    "                case \"T\": self.next_pieces_threehot[i,5] = 1\n",
    "                case \"Z\": self.next_pieces_threehot[i,6] = 1\n",
    "        self.next_pieces_threehot = self.next_pieces_threehot.flatten()\n",
    "        #print(self.next_pieces_threehot, flush=True)\n",
    "\n",
    "        # Incoming Severity Input\n",
    "        incoming = input().split()\n",
    "        self.incoming_severity = np.array([min(int(incoming[1]), 20) / 20.0], dtype=np.float32)\n",
    "        #print(self.incoming_severity, flush=True)\n",
    "\n",
    "        \"\"\"\n",
    "        self.observation = np.concatenate([\n",
    "            self.board_arr,          # 0:199\n",
    "            self.placing_board_arr,  # 200:399\n",
    "            self.placing_onehot,     # 400:406\n",
    "            self.rotation_onehot,\n",
    "            self.pos,\n",
    "            self.held_onehot,\n",
    "            self.hasheld_bit,\n",
    "            self.next_pieces_threehot,\n",
    "            self.incoming_severity\n",
    "        ])\n",
    "        \"\"\"\n",
    "        #print(\"Observation:\")\n",
    "        #print(self.observation, flush=True)\n",
    "\n",
    "        # Simplified Observation Set\n",
    "        self.observation = np.concatenate([\n",
    "            self.board_arr,          # 0:199\n",
    "            self.placing_board_arr  # 200:399\n",
    "        ])\n",
    "\n",
    "        return True\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Currently Implemented Reward Factors:\n",
    "    - Gameover:   -7.5\n",
    "    - Default:    0\n",
    "        - Placed:     += 0.5\n",
    "        - Max Height: -= 0.1*(# of rows)\n",
    "        - Overhang:   -= max(-0.75, 0.05 - (0.25 * piece_overhang)) # That is, 0.05 minus 0.25 per tile of overhang, minimum of -=0.75\n",
    "        - Lines:      += 1*(# lines cleared)\n",
    "        - Sent:       += 1*(# lines sent)\n",
    "        - B2B:        += 1\n",
    "        - Combo:      += 0.5*(Combo)\n",
    "        - Invalids    -= 0.15\n",
    "        - Repeats     -= 0.2\n",
    "\n",
    "    Disabled:\n",
    "    - Hole Creation:  -= 0.25\n",
    "    \"\"\"\n",
    "    def step(self, action):\n",
    "        self.stepbegin_timestamp = time.perf_counter()\n",
    "\n",
    "        if self.stepend_timestamp != None:\n",
    "            step_interim = self.stepbegin_timestamp - self.stepend_timestamp\n",
    "            print(f\"Next step starting after interim of: {(step_interim*1000):.4f} ms (since previous step ended)\")\n",
    "        \n",
    "        print(\"Stepping!\", flush=True)\n",
    "\n",
    "        \n",
    "        self.stdiobegin_timestamp = time.perf_counter()\n",
    "        strin = input()    # receive move\n",
    "        if strin == \"move\": print(\"Got move order\")\n",
    "        \"\"\"\n",
    "        # Full Action Set\n",
    "        match action:\n",
    "            case 0: print(\"move left\", flush=True)\n",
    "            case 1: print(\"move right\", flush=True)\n",
    "            case 2: print(\"move hard\", flush=True)\n",
    "            case 3: print(\"move soft\", flush=True)\n",
    "            case 4: print(\"move cw\", flush=True)\n",
    "            case 5: print(\"move ccw\", flush=True)\n",
    "            case 6: print(\"move hold\", flush=True)\n",
    "        \"\"\"\n",
    "\n",
    "        # Simplified Action Set\n",
    "        match action:\n",
    "            case 0: print(\"move left\", flush=True)\n",
    "            case 1: print(\"move right\", flush=True)\n",
    "            case 2: print(\"move cw\", flush=True)\n",
    "            case 3: print(\"move soft\", flush=True)\n",
    "\n",
    "        self.reward = 0\n",
    "\n",
    "        strin = input()   # receive ack\n",
    "        strin = input()   # receive \"gameover\" or first line of report (`lines`)\n",
    "        if(strin == \"gameover\"):\n",
    "            #print(\"Gameovered. Rip.\")\n",
    "            self.terminated = True\n",
    "            self.reward -= 7.5                        # penalty for dying\n",
    "            self.previous_observation = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "            self.observation = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        else:\n",
    "            lines = int(strin.split()[1])\n",
    "            sent = int(input().split()[1])\n",
    "            b2b = True if input().split()[1]==\"true\" else False\n",
    "            combo = int(input().split()[1])\n",
    "            invalidmove = True if input().split()[1]==\"true\" else False\n",
    "            repeatedmove = True if input().split()[1]==\"true\" else False\n",
    "            \n",
    "            self.renderbegin_timestamp = time.perf_counter()\n",
    "            print(\"ready\", flush=True)\n",
    "            strin = input()    # receive ack\n",
    "            print(strin, flush=True)\n",
    "            \n",
    "            \n",
    "            self.save_previous_observation_components()\n",
    "            self.previous_observation = self.observation\n",
    "            self.parse_observations()\n",
    "\n",
    "            self.stdioend_timestamp = time.perf_counter()\n",
    "            stdio_time = self.stdioend_timestamp - self.stdiobegin_timestamp\n",
    "            print(f\"Doing the important stdio things took: {(stdio_time*1000):.4f} ms\")\n",
    "            \n",
    "\n",
    "\n",
    "            # If a piece was placed\n",
    "            piece_changed = not np.array_equal(self.next_pieces_threehot, self.prev_next_pieces_threehot)\n",
    "            #print(self.previous_observation[455:476])\n",
    "            #print(self.observation[455:476])\n",
    "            if piece_changed:\n",
    "                prev_board = self.previous_observation[0:200].reshape((20, 10))\n",
    "                curr_board = self.observation[0:200].reshape((20, 10))\n",
    "                #print(prev_board, \"\\n\")\n",
    "                #print(curr_board)\n",
    "\n",
    "                \n",
    "                # Decide whether the piece was placed in a position that raises the max board height\n",
    "                prev_rows_with_ones = np.where(prev_board.any(axis=1))[0]\n",
    "                if len(prev_rows_with_ones) > 0:\n",
    "                    prev_bottommost_row = prev_rows_with_ones[-1]\n",
    "                else: prev_bottommost_row = 0;\n",
    "                #print(f\"prev Bottommost filled row index: {prev_bottommost_row}\")\n",
    "\n",
    "                curr_rows_with_ones = np.where(curr_board.any(axis=1))[0]\n",
    "                if len(curr_rows_with_ones) > 0:\n",
    "                    curr_bottommost_row = curr_rows_with_ones[-1]\n",
    "                else: curr_bottommost_row = 0;\n",
    "                #print(f\"curr Bottommost filled row index: {curr_bottommost_row}\")\n",
    "\n",
    "                self.reward += 0.5    # Base reward for placing a piece (before poor placement penalties)\n",
    "                \n",
    "                # Penalize if the placed piece increased row height (don't penalize if no pieces placed yet\n",
    "                if prev_bottommost_row != 0 and curr_bottommost_row > prev_bottommost_row:\n",
    "                    penalty = ((curr_bottommost_row - prev_bottommost_row) * 0.1)\n",
    "                    print(\"Board height penalty assigned: \", penalty)\n",
    "                    self.reward -= penalty\n",
    "\n",
    "\n",
    "                # Decide whether the piece was placed in a position that created more holes in the board\n",
    "                prev_holes = self.count_enclosed_regions(prev_board)\n",
    "                curr_holes = self.count_enclosed_regions(curr_board)\n",
    "                #print(\"Previous enclosed regions: \", prev_holes)\n",
    "                #print(\"Current enclosed regions: \", curr_holes)\n",
    "                if curr_holes > prev_holes:\n",
    "                    self.holes_created += 1\n",
    "                    #print(\"Assigning hole-creation penalty\")\n",
    "                    #self.reward -= 0.25\n",
    "\n",
    "                # Decide how much overhang the placed piece caused\n",
    "                placed_coords = np.logical_xor(prev_board, curr_board).astype(int)\n",
    "                #print(\"Placed coords:\")\n",
    "                #print(placed_coords)\n",
    "                piece_overhang = 0\n",
    "                for row, col in np.argwhere(placed_coords):\n",
    "                    col_overhang = 0\n",
    "                    while row > 0:\n",
    "                        row -= 1\n",
    "                        if curr_board[row,col] == 0:\n",
    "                            piece_overhang += 1\n",
    "                            col_overhang += 1\n",
    "                        else: break\n",
    "                    #print(\"Overhang in col \", col, \"1: \", col_overhang)\n",
    "                print(\"Piece Overhang: \", piece_overhang)\n",
    "                overhang_reward = max(-0.75, 0.05 - (0.25 * piece_overhang))  # Reward overhang 0, neutral == 1, penalize > 1. Max penalty of -1\n",
    "                print(\"  rew/pen=\", overhang_reward)\n",
    "                self.reward +=  overhang_reward   \n",
    "                self.total_overhang += piece_overhang\n",
    "                \n",
    "                #time.sleep(1)\n",
    "                \n",
    "                \n",
    "\n",
    "                self.reward += lines                          # +1 reward per line cleared\n",
    "                self.reward += sent                           # +1 reward per line sent\n",
    "                self.reward += (1 if b2b else 0)              # +1 reward if b2b'd\n",
    "                self.reward += combo/2.0                      # +0.5 reward per combo\n",
    "                self.reward -= (0.15 if invalidmove else 0.0)  # -0.1 reward if invalid move\n",
    "                self.reward -= (0.2 if repeatedmove else 0.0) # -0.1 reward if repeated move (avoid, but if you need a repeated for some reason it's ok)\n",
    "    \n",
    "                #self.reward /= 5   # normalize, maybe\n",
    "                \n",
    "                if lines > 0:\n",
    "                    self.lines_cleared += lines\n",
    "                if invalidmove:\n",
    "                    self.invalid_moves += 1\n",
    "                if repeatedmove:\n",
    "                    self.repeated_moves += 1\n",
    "                if piece_changed:\n",
    "                    self.pieces_placed += 1\n",
    "            # To implement:\n",
    "            # + reward for placing blocks without any gaps below them\n",
    "            # - penalty for placing blocks with overhang (inverse of above)\n",
    "            # + reward for filling partially-filled lines\n",
    "            # + small reward for placing a piece (survival)\n",
    "            #print(\"Step complete, rewards assigned.\", flush=True)\n",
    "\n",
    "    \n",
    "\n",
    "        info = {\"lines\": self.lines_cleared,\n",
    "                \"pieces\": self.pieces_placed,\n",
    "                \"repeats\": self.repeated_moves,\n",
    "                \"invalids\": self.invalid_moves,\n",
    "                \"holes_created\": self.holes_created,\n",
    "                \"total_overhang\": self.total_overhang}\n",
    "\n",
    "        \"\"\"\n",
    "        # Override test: Sort out O-Pieces rightward\n",
    "        if self.previous_observation[403]:\n",
    "            if action == 1: self.reward = 5\n",
    "            else: self.reward = -5\n",
    "        else:\n",
    "            if action == 0: self.reward = 5\n",
    "            else: self.reward = -5\n",
    "        \"\"\"\n",
    "\n",
    "        self.stepend_timestamp = time.perf_counter()\n",
    "        step_time = self.stepend_timestamp - self.stepbegin_timestamp\n",
    "        print(f\"Full step took: {(step_time*1000):.4f} ms\")\n",
    "\n",
    "        return self.observation, self.reward, self.terminated, self.truncated, info\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self, seed = None):\n",
    "        print(\"Resetting!\", flush=True)\n",
    "\n",
    "        self.reward = 0.0\n",
    "        self.terminated = False\n",
    "        self.truncated = False\n",
    "        self.lines_cleared = 0\n",
    "        self.pieces_placed = 0\n",
    "        self.invalid_moves = 0\n",
    "        self.repeated_moves = 0\n",
    "        self.holes_created = 0\n",
    "        self.total_overhang = 0\n",
    "\n",
    "\n",
    "        #if not self.firstGame:\n",
    "        #    print(\"First game, doing second options pass and waiting for ack\", flush=True)\n",
    "        #    print(\"options seed 1\", flush=True)\n",
    "        #    strin = input()    # Receive gameover\n",
    "        #else: self.firstGame = False\n",
    "\n",
    "        print(\"options seed 1\", flush=True)\n",
    "\n",
    "        self.renderbegin_timestamp = time.perf_counter()\n",
    "        print(\"ready\", flush=True)\n",
    "        strin = input()    # Receive ack\n",
    "        #print(strin)\n",
    "\n",
    "        self.previous_observation = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        self.parse_observations()\n",
    "\n",
    "        info = {}\n",
    "        return self.observation, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a188d-8c95-4718-a113-0b7cf60bb6f7",
   "metadata": {},
   "source": [
    "###### Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c8545-1721-469c-ba5c-909c35e9cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines3.common.env_checker import check_env\n",
    "#env = TetrisEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ae2e2-e803-40cb-ba0e-9ac006c797a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb1af2-fabe-453e-a48c-d116873dc75f",
   "metadata": {},
   "source": [
    "## Define CNN-MLP Hybrid Feature Extractor\n",
    "(Written by ChatGPT)\n",
    "\n",
    "(Overridden to be minimal, CNN-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797337a7-fba8-443d-8471-7ea3cd871653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class TetrisFeatureExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Uses ONLY the 2-channel CNN input (20x10x2 board).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, features_dim: int = 256):\n",
    "        # We will overwrite features_dim later after computing cnn_out_size\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # -------------------------------\n",
    "        # CNN branch: 2-channel 20x10 board\n",
    "        # -------------------------------\n",
    "        self.board_cnn = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # compute flattened CNN output size\n",
    "        with th.no_grad():\n",
    "            dummy_input = th.zeros((1, 2, 20, 10))\n",
    "            cnn_out_size = self.board_cnn(dummy_input).shape[1]\n",
    "\n",
    "        # Replace the final layer to output desired feature dimension\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(cnn_out_size, features_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Update features_dim so SB3 knows the output shape\n",
    "        self._features_dim = features_dim\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        # Extract only the board channels (2 * 200 = 400 values)\n",
    "        board_data = observations[:, :400].reshape((-1, 2, 20, 10))\n",
    "\n",
    "        board_features = self.board_cnn(board_data)\n",
    "        return self.final(board_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8796f-c0e9-43ba-8726-356bc55af297",
   "metadata": {},
   "source": [
    "## Initialize Environment and Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a71e0-7362-4d8f-987b-5caeb9eec097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "\tos.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "\tos.makedirs(logdir)\n",
    "\n",
    "env = Monitor(TetrisEnv())\n",
    "env = DummyVecEnv([lambda: env])\n",
    "#env.reset()\n",
    "print(\"Reset done!\")\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=TetrisFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    "    net_arch=[dict(pi=[128, 64], vf=[128, 64])]\n",
    ")\n",
    "\n",
    "\n",
    "cfg_path = 'model.cfg'\n",
    "\n",
    "# If model config file is empty, initialize empty model. Otherwise, load model\n",
    "if os.path.getsize(cfg_path) == 0:\n",
    "    print(\"Initializing untrained model\")\n",
    "    model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    device = \"cuda\",\n",
    "    tensorboard_log=logdir,\n",
    "    gamma=0.99,\n",
    "    seed=0\n",
    "    )\n",
    "else:\n",
    "    try:\n",
    "        with open(cfg_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            content = content.strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Config file {cfg_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    try: \n",
    "        print(\"Loading trained model from \" + content)\n",
    "        model = PPO.load(content, env=env, device=\"cuda\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model {content} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d9ac19-bc1e-4c52-84ea-5e4e3ed6591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a93ab1-7e89-4af4-a98b-2ca0d5a14da5",
   "metadata": {},
   "source": [
    "## Define Logger Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b418cd6-7cab-4e72-a15e-813dd1d9e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class LoggerCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.locals['dones'][0]:\n",
    "            infos = self.locals.get(\"infos\", [])\n",
    "            for info in infos:\n",
    "                if \"lines\" in info:\n",
    "                    self.logger.record(\"env/lines\", info[\"lines\"])\n",
    "                if \"pieces\" in info:\n",
    "                    self.logger.record(\"env/pieces\", info[\"pieces\"])\n",
    "                if \"invalids\" in info:\n",
    "                    self.logger.record(\"env/invalids\", info[\"invalids\"])\n",
    "                if \"repeats\" in info:\n",
    "                    self.logger.record(\"env/repeats\", info[\"repeats\"])\n",
    "                if \"holes_created\" in info:\n",
    "                    self.logger.record(\"env/holes_created\", info[\"holes_created\"])\n",
    "                if \"total_overhang\" in info and info[\"pieces\"] != 0:\n",
    "                    self.logger.record(\"env/avg_overhang\", info[\"total_overhang\"]/info[\"pieces\"])\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582d310-0903-45e4-99b4-40ed7416ec7e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b3bdc-1c9f-469a-8316-383adbd14045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "TIMESTEPS = 1e5;      # Train indefinitely, saving every 100k timesteps\n",
    "iters = 0\n",
    "callback = LoggerCallback()\n",
    "\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        iters+=1\n",
    "        model.learn(\n",
    "            total_timesteps=TIMESTEPS,\n",
    "            reset_num_timesteps=False,\n",
    "            callback = callback,\n",
    "            tb_log_name=f\"TetoPPO\",\n",
    "            progress_bar=False)\n",
    "        model.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n",
    "    print(\"Training complete, saving model.\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS}\")\n",
    "except Exception:\n",
    "    print(\"Python error:\")\n",
    "    print(traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
